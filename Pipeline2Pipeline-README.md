
# Harness Pipeline - Passing Variables Between Pipelines

## 1. Using Artifacts to Pass Data Between Pipelines

One of the common ways to pass data is to use **Artifacts** (files or outputs) generated by one pipeline and consumed by another.

### Steps:
1. **Pipeline A** (Producer): Generate or store the data (artifact).
2. **Pipeline B** (Consumer): Fetch the artifact and use it as input.

### Pipeline A (Producer): Save Data as an Artifact

```yaml
pipeline:
  name: "Pipeline A - Generate Data"
  identifier: "Pipeline_A"
  projectIdentifier: "Your_Project"
  orgIdentifier: "Your_Org"
  stages:
    - stage:
        name: "Generate Artifact"
        identifier: "Generate_Artifact"
        type: "CI"
        spec:
          execution:
            steps:
              - step:
                  name: "Generate Artifact"
                  identifier: "Generate_Artifact_Step"
                  type: "ShellScript"
                  spec:
                    shell: Sh
                    script: |
                      echo "This is a test data" > /harness/test_data.txt
              - step:
                  name: "Upload Artifact to Harness"
                  identifier: "Upload_Artifact"
                  type: "StoreArtifact"
                  spec:
                    connectorRef: "your-artifact-repository"
                    artifactPaths:
                      - /harness/test_data.txt
                    artifactName: "test_data"
                    type: "Generic"
```

In this example, **Pipeline A** generates an artifact (`test_data.txt`) and uploads it to Harness' **Artifact Store**.

### Pipeline B (Consumer): Fetch the Artifact and Use the Data

```yaml
pipeline:
  name: "Pipeline B - Consume Data"
  identifier: "Pipeline_B"
  projectIdentifier: "Your_Project"
  orgIdentifier: "Your_Org"
  stages:
    - stage:
        name: "Fetch and Use Artifact"
        identifier: "Fetch_Artifact"
        type: "Deployment"
        spec:
          execution:
            steps:
              - step:
                  name: "Fetch Artifact"
                  identifier: "Fetch_Artifact_Step"
                  type: "ArtifactSource"
                  spec:
                    connectorRef: "your-artifact-repository"
                    artifactName: "test_data"
                    type: "Generic"
              - step:
                  name: "Use the Artifact Data"
                  identifier: "Use_Artifact_Data"
                  type: "ShellScript"
                  spec:
                    shell: Sh
                    script: |
                      cat ${artifact.test_data}
                      # Process the data here
```

In **Pipeline B**, the **`ArtifactSource`** step fetches the artifact (`test_data`) generated by **Pipeline A**, and the data can be used in subsequent steps.

## 2. Using Variables to Pass Values Between Pipelines (Triggering Pipelines)

You can pass values between pipelines by using **Pipeline Variables**. You can trigger a second pipeline from the first and pass variables dynamically.

### Trigger Pipeline B from Pipeline A:

1. **Pipeline A** will trigger **Pipeline B** at the end.
2. **Pipeline A** will pass any variable values as part of the trigger.

**Example: Pipeline A Triggering Pipeline B**
```yaml
pipeline:
  name: "Pipeline A - Trigger Pipeline B"
  identifier: "Pipeline_A"
  projectIdentifier: "Your_Project"
  orgIdentifier: "Your_Org"
  stages:
    - stage:
        name: "Trigger Pipeline B"
        identifier: "Trigger_Pipeline_B"
        type: "CI"
        spec:
          execution:
            steps:
              - step:
                  name: "Trigger Pipeline B"
                  identifier: "Trigger_Pipeline_B_Step"
                  type: "RunPipeline"
                  spec:
                    pipeline:
                      identifier: "Pipeline_B"
                      projectIdentifier: "Your_Project"
                      orgIdentifier: "Your_Org"
                    variables:
                      PARAMETER_FROM_PIPELINE_A: "<+pipeline.variables.PARAMETER_TO_PASS>"
```

Here, **Pipeline A** triggers **Pipeline B** and passes a variable `PARAMETER_TO_PASS` to **Pipeline B**.

**In Pipeline B**, you can access the variable as:

```yaml
pipeline:
  name: "Pipeline B - Use Passed Variables"
  identifier: "Pipeline_B"
  projectIdentifier: "Your_Project"
  orgIdentifier: "Your_Org"
  stages:
    - stage:
        name: "Use Passed Variable"
        identifier: "Use_Variable"
        type: "CI"
        spec:
          execution:
            steps:
              - step:
                  name: "Use Variable"
                  identifier: "Use_Variable_Step"
                  type: "ShellScript"
                  spec:
                    shell: Sh
                    script: |
                      echo "Received value from Pipeline A: ${PARAMETER_FROM_PIPELINE_A}"
```

## 3. Using Shared Services (e.g., Secrets, Configurations)

Another way to share data between pipelines is by using shared services such as **Secrets** (e.g., API keys, database credentials) or **Configuration Files** stored in an artifact repository or shared location.

- Store data in a **shared location** (e.g., a **S3 bucket**, **Database**, or **Config Server**).
- Fetch the shared data in the target pipeline.

## Summary

- **Artifacts**: Use artifacts to pass actual files (e.g., logs, configuration files) between pipelines.
- **Pipeline Variables**: Use variables to pass dynamic values (e.g., version numbers, parameters) from one pipeline to another when triggering pipelines.
- **Secrets/Configuration Services**: Use shared services like **Secrets Manager** or **config files** for shared sensitive data.
